{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV1PFH6Jf1_i"
      },
      "source": [
        "# End-to-End LLM based solution\n",
        "Using sentence semantic symilarity and summaring models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests and Tries"
      ],
      "metadata": {
        "id": "VDF2wk-xUbdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9WTf-xQ5hwfX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9mppF9O0fp_D"
      },
      "outputs": [],
      "source": [
        "COLAB = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHAc5GlkgJas",
        "outputId": "72b438cf-3bc6-4f82-b9e1-a4e60c1e02fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    from os.path import exists\n",
        "\n",
        "    amazon_train_file = 'train.ft.txt.bz2'\n",
        "\n",
        "    if exists(amazon_train_file):\n",
        "        train_file = amazon_train_file\n",
        "    else:\n",
        "        drive.mount('/content/drive/')\n",
        "\n",
        "        # For Hadas' drive\n",
        "        my_dir = 'drive/MyDrive/Y-data/Intuit-K-anonimity/'\n",
        "\n",
        "        # For Lior's drive\n",
        "        #my_dir = 'drive/MyDrive/Y-data/Y-DATA_PROJECT/'\n",
        "\n",
        "        train_file = my_dir + '/train.ft.txt.bz2'\n",
        "else:\n",
        "    train_file = '../data/' + 'train.ft.txt.bz2'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kvd3SEkEgNNA"
      },
      "outputs": [],
      "source": [
        "# Credit https://www.kaggle.com/code/anshulrai/cudnnlstm-implementation-93-7-accuracy\n",
        "\n",
        "import bz2\n",
        "\n",
        "# Readling the file to list of comments\n",
        "train_file = bz2.BZ2File(train_file)\n",
        "train_file_lines = train_file.readlines()\n",
        "\n",
        "# Converting from raw binary strings to strings that can be parsed\n",
        "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
        "\n",
        "# Extracting the labels and sentences\n",
        "#train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
        "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines] # And converting to lower case\n",
        "\n",
        "del(train_file_lines)  # Free RAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlIuiP2cgQ_t",
        "outputId": "acd48d0e-7d8f-45b3-ce8f-60c15b36dce6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3600000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(train_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTCQGFkgjti"
      },
      "source": [
        "### Working with short sentences - temporarly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNSnYyBOgo-9",
        "outputId": "43bbba34-92f7-4e8e-8eb3-4be5dc75c485"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28005"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "short_train_sentences = [x for x in train_sentences if len(x.split(' ')) < 20]\n",
        "len(short_train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5vGg5XGUg3I9"
      },
      "outputs": [],
      "source": [
        "train_sentences, all_sentences = short_train_sentences, train_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfHyLIe1gWYD"
      },
      "source": [
        "### Sentences similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hohLiUvTgY2Y"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0kp9Zk4RgePd"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCLoD8lKhcP_"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7s-i1iEge_x",
        "outputId": "7cea1624-ae27-4184-fc71-f7f68028774a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.99 s, sys: 181 ms, total: 6.17 s\n",
            "Wall time: 8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\n",
        "embeddings = model.encode(train_sentences[:100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-arNL6H0hnYy"
      },
      "source": [
        "Finding the most similar sentences to the first sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh8ybzY9hdcz",
        "outputId": "1fc3282c-37a3-48d3-9315-3fe4b3a6219a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.0000001, 0), (0.6311436, 32), (0.55656093, 45), (0.46512488, 67)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "one_sent = embeddings[0]\n",
        "sim = []\n",
        "idx, max_idx = 1, 1\n",
        "for idx, e in enumerate(embeddings[:]):\n",
        "    cos_sim = np.dot(e, one_sent)/(np.linalg.norm(e)*np.linalg.norm(one_sent))\n",
        "    sim.append((cos_sim, idx))\n",
        "\n",
        "sim.sort(reverse=True)\n",
        "sim[0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QR186GDaitWt",
        "outputId": "efec2709-d617-48c1-b6b8-1adc517d6d84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'textbook: book shipped quickly and was in excellent condition as stated. easy transaction would buy again'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "u5zCoYsOixqH",
        "outputId": "2fc1ee24-7b16-4960-acd2-13087df4a2fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'recent purchase: the book i ordered was exactly as described and delivery was even faster than promised.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_sentences[32]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b62zjzERjX7p"
      },
      "source": [
        "Finding K nearest neighbors using Annoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8clGb05j7Lx",
        "outputId": "007eca7e-4f73-4523-a92d-b78d77cfdda7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.9/dist-packages (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install annoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xi0j_zjSjqQW"
      },
      "outputs": [],
      "source": [
        "k = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiugQns5iz_m",
        "outputId": "fb6ff13b-f773-42fc-bbcb-dd1f6f140367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors: [0, 32, 45]\n"
          ]
        }
      ],
      "source": [
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Build an Annoy index with 10 trees. angular = cosine similarity\n",
        "annoy_index = AnnoyIndex(embeddings.shape[1], metric='angular')\n",
        "for i, x in enumerate(embeddings):\n",
        "    annoy_index.add_item(i, x)\n",
        "annoy_index.build(10)\n",
        "\n",
        "# Find the k nearest neighbors to the first sentence\n",
        "nearest_neighbors = annoy_index.get_nns_by_vector(embeddings[0], k)\n",
        "\n",
        "print('Nearest neighbors:', nearest_neighbors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9dfedCUkbi1",
        "outputId": "d3eb3f59-6d22-44b9-f6f4-fb418c08fe53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textbook: book shipped quickly and was in excellent condition as stated. easy transaction would buy again\n",
            "recent purchase: the book i ordered was exactly as described and delivery was even faster than promised.\n",
            "recommend this seller: i recieved this book on time and in excellent condition. i'd definitely recommend this seller.\n"
          ]
        }
      ],
      "source": [
        "print(train_sentences[0])\n",
        "print(train_sentences[32])\n",
        "print(train_sentences[45])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQDNKaefk65r"
      },
      "source": [
        "### Summaring multiple sentences into one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xFqOedVKlAOy"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# model_name = \"snrspeaks/t5-one-line-summary\"\n",
        "# sum_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lhmWvgK_lMqt"
      },
      "outputs": [],
      "source": [
        "#sum_text([train_sentences[0], train_sentences[32], train_sentences[45]], sum_model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obI_MRRvlbkY"
      },
      "source": [
        "## Putting it all together - function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUAQMIUOmi_-",
        "outputId": "f9e75d80-e24a-4b19-aa8f-07db51b55bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 643 ms, sys: 126 ms, total: 769 ms\n",
            "Wall time: 860 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h_LXxYUY0w9F"
      },
      "outputs": [],
      "source": [
        "def print_example(indexes):\n",
        "    print('Before:')\n",
        "    for i in indexes:\n",
        "        print(train_sentences[i])\n",
        "    print('After:')\n",
        "    for i in indexes:\n",
        "        print(annon_sents[i])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-GdlQExPmCiq"
      },
      "outputs": [],
      "source": [
        "def find_k_nearesr_neighbors(emb, emb_list, k):\n",
        "    \"\"\" Find the K nearest neighbors using Annoy package \"\"\"\n",
        "\n",
        "    # Build an Annoy index with 10 trees. angular = cosine similarity\n",
        "    annoy_index = AnnoyIndex(emb_list.shape[1], metric='angular')\n",
        "    for i, x in enumerate(emb_list):\n",
        "        annoy_index.add_item(i, x)\n",
        "    annoy_index.build(10)\n",
        "\n",
        "    # Find the k nearest neighbors to the first sentence\n",
        "    nearest_neighbors = annoy_index.get_nns_by_vector(emb, k)\n",
        "\n",
        "    return nearest_neighbors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
        "\n",
        "def sum_text(doc_list):\n",
        "    # define the input sentences\n",
        "    #input_text = '. '.join(doc_list)\n",
        "    input_text = ''\n",
        "    i = 1\n",
        "    for doc in doc_list:\n",
        "       input_text = f'{input_text}{i}: {doc}. ' \n",
        "       i += 1\n",
        "\n",
        "    # preprocess the input sentences\n",
        "    input_ids = tokenizer.encode(f'summarize the {i} documents:' + input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # generate the summary sentence\n",
        "    output_ids = model.generate(input_ids=input_ids, max_length=32, num_beams=4, early_stopping=True)\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t-Oh4BsJqZC",
        "outputId": "ca24d96c-2e9c-4093-e74b-40596c95884f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9aVhh-X1m2Jq"
      },
      "outputs": [],
      "source": [
        "def run_anonymization_on_txt(docs, k):\n",
        "    \"\"\" Finding K nearest neighbors and summarize them \"\"\"\n",
        "    annon_docs = docs.copy()\n",
        "    used_indexes = set([])\n",
        "    \n",
        "    # Embedding\n",
        "    docs_emb = emb_model.encode(docs)\n",
        "    temp_docs_emb = docs_emb.copy()\n",
        "\n",
        "    for i, d in enumerate(docs):\n",
        "        #print('i:', i, '\\t', used_indexes)\n",
        "        # To prevent redandent\n",
        "        if i not in used_indexes:\n",
        "            used_indexes.add(i)  # Adding to the used items\n",
        "            similar_doc_ind = find_k_nearesr_neighbors(temp_docs_emb[i], temp_docs_emb, k)\n",
        "            print('similar_doc_ind', similar_doc_ind)\n",
        "            curr_docs = []\n",
        "            for sd in similar_doc_ind:\n",
        "                # Adding the document to the similar doc list\n",
        "                curr_docs.append(docs[sd])\n",
        "                # Adding the index to the used items\n",
        "                used_indexes.add(sd)  \n",
        "                # Prevent repeating comparison by changing the vector\n",
        "                temp_docs_emb[sd] = 1000 * np.random.randint(10, size=len(temp_docs_emb[sd]))\n",
        "                #temp_docs_emb[sd] = [1000] * len(temp_docs_emb[sd])\n",
        "            sum_doc = sum_text(curr_docs)\n",
        "            #print('sum_doc:', sum_doc)\n",
        "            for sd in similar_doc_ind:\n",
        "                annon_docs[sd] = sum_doc\n",
        "        if  len(used_indexes) > (len(docs) - k):\n",
        "            print('Breaking! \\tlen(used_indexes)', len(used_indexes), '\\tlen(docs)', len(docs), '\\tlen(docs)-k', (len(docs) - k))\n",
        "            break\n",
        "    return annon_docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7JgzJm-UxDx"
      },
      "source": [
        "## Running on 100 examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WaKKrwmpGqb",
        "outputId": "05e5c0d0-721a-4af8-9a08-684924b96183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similar_doc_ind [0, 32, 45]\n",
            "similar_doc_ind [1, 72, 73]\n",
            "similar_doc_ind [2, 63, 41]\n",
            "similar_doc_ind [3, 49, 56]\n",
            "similar_doc_ind [4, 35, 47]\n",
            "similar_doc_ind [5, 6, 80]\n",
            "similar_doc_ind [7, 36, 37]\n",
            "similar_doc_ind [8, 50, 12]\n",
            "similar_doc_ind [9, 92, 86]\n",
            "similar_doc_ind [10, 26, 70]\n",
            "similar_doc_ind [11, 54, 40]\n",
            "similar_doc_ind [13, 88, 82]\n",
            "similar_doc_ind [14, 69, 29]\n",
            "similar_doc_ind [15, 30, 24]\n",
            "similar_doc_ind [16, 85, 94]\n",
            "similar_doc_ind [17, 34, 98]\n",
            "similar_doc_ind [18, 59, 83]\n",
            "similar_doc_ind [19, 78, 87]\n",
            "similar_doc_ind [20, 90, 28]\n",
            "similar_doc_ind [21, 33, 25]\n",
            "similar_doc_ind [22, 64, 39]\n",
            "similar_doc_ind [23, 99, 55]\n",
            "similar_doc_ind [27, 42, 95]\n",
            "similar_doc_ind [31, 96, 43]\n",
            "similar_doc_ind [38, 76, 58]\n",
            "similar_doc_ind [44, 57, 79]\n",
            "similar_doc_ind [46, 93, 66]\n",
            "similar_doc_ind [48, 61, 74]\n",
            "similar_doc_ind [51, 53, 52]\n",
            "similar_doc_ind [60, 67, 75]\n",
            "similar_doc_ind [62, 81, 91]\n",
            "similar_doc_ind [65, 97, 68]\n",
            "similar_doc_ind [71, 77, 84]\n",
            "Breaking! \tlen(used_indexes) 99 \tlen(docs) 100 \tlen(docs)-k 97\n",
            "CPU times: user 7min 21s, sys: 946 ms, total: 7min 22s\n",
            "Wall time: 7min 38s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "annon_sents = run_anonymization_on_txt(train_sentences[:100], k=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "4A77H5M3jeyT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUPuf_x30qfB"
      },
      "source": [
        "Output examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbNOQjiJ1JAW",
        "outputId": "bba6bf12-4539-4180-8bbd-6ac0ccecc599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "textbook: book shipped quickly and was in excellent condition as stated. easy transaction would buy again\n",
            "recent purchase: the book i ordered was exactly as described and delivery was even faster than promised.\n",
            "recommend this seller: i recieved this book on time and in excellent condition. i'd definitely recommend this seller.\n",
            "After:\n",
            "i recieved this book on time and in excellent condition. i'd definitely recommend this seller.\n",
            "i recieved this book on time and in excellent condition. i'd definitely recommend this seller.\n",
            "i recieved this book on time and in excellent condition. i'd definitely recommend this seller.\n"
          ]
        }
      ],
      "source": [
        "print_example([0, 32, 45])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysQRIvcP1Kop",
        "outputId": "56d1c798-f1bc-4f16-fb7b-076f3a105825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "janes all the worlds aircraft 1996-7: great to deal with. very quick delivery. very highly recommended. thank you.\n",
            "good bike, bad packing: when i received the bike, the packing board was broken.the bike quality is good.\n",
            "great purchase: installed in seconds. great performance. excellent to eliminate phone line dependency.\n",
            "After:\n",
            "janes all the worlds aircraft 1996-7: great deal with very quick delivery. when i received the bike, the packing board was broken.\n",
            "janes all the worlds aircraft 1996-7: great deal with very quick delivery. when i received the bike, the packing board was broken.\n",
            "janes all the worlds aircraft 1996-7: great deal with very quick delivery. when i received the bike, the packing board was broken.\n"
          ]
        }
      ],
      "source": [
        "print_example([1, 72, 73])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk3D3gfa1MWA",
        "outputId": "cf4085c1-c684-4fff-bf73-a51ac1ed5ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "beach boys and the satan: a totally off and enjoyable movie. all brian wilson fans should see this movie.\n",
            "great: one of adams best films along side the wedding singer and mr. deeds. funny movie. worth buying\n",
            "solid follow up: very good movie. for children 7 and up...any younger they may not grasp the concept.\n",
            "After:\n",
            "beach boys and the satan: a totally off and enjoyable movie. one of adams best films along side the wedding singer\n",
            "beach boys and the satan: a totally off and enjoyable movie. one of adams best films along side the wedding singer\n",
            "beach boys and the satan: a totally off and enjoyable movie. one of adams best films along side the wedding singer\n"
          ]
        }
      ],
      "source": [
        "print_example([19, 78, 87])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWI7fR0n9A4h"
      },
      "source": [
        "Test anonymity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZAJSYWbH8_c2"
      },
      "outputs": [],
      "source": [
        "#get_pesonal_docs(annon_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr-5cyohqTsE"
      },
      "source": [
        "## Running on 1000 examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ09ur7goPcu",
        "outputId": "94731745-542a-4ed9-ba6d-4aa78606fd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similar_doc_ind [0, 32, 178]\n",
            "similar_doc_ind [1, 200, 487]\n",
            "similar_doc_ind [2, 393, 158]\n",
            "similar_doc_ind [3, 367, 234]\n",
            "similar_doc_ind [4, 243, 285]\n",
            "similar_doc_ind [5, 126, 332]\n",
            "similar_doc_ind [6, 312, 403]\n",
            "similar_doc_ind [7, 264, 274]\n",
            "similar_doc_ind [8, 410, 395]\n",
            "similar_doc_ind [9, 137, 136]\n",
            "similar_doc_ind [10, 111, 326]\n",
            "similar_doc_ind [11, 462, 106]\n",
            "similar_doc_ind [12, 218, 186]\n",
            "similar_doc_ind [13, 339, 88]\n",
            "similar_doc_ind [14, 486, 224]\n",
            "similar_doc_ind [15, 92, 157]\n",
            "similar_doc_ind [16, 30, 335]\n",
            "similar_doc_ind [17, 146, 180]\n",
            "similar_doc_ind [18, 475, 63]\n",
            "similar_doc_ind [19, 370, 470]\n",
            "similar_doc_ind [20, 344, 153]\n",
            "similar_doc_ind [21, 152, 352]\n",
            "similar_doc_ind [22, 213, 302]\n",
            "similar_doc_ind [23, 366, 210]\n",
            "similar_doc_ind [24, 474, 305]\n",
            "similar_doc_ind [25, 163, 286]\n",
            "similar_doc_ind [26, 235, 303]\n",
            "similar_doc_ind [27, 293, 471]\n",
            "similar_doc_ind [28, 62, 70]\n",
            "similar_doc_ind [29, 40, 328]\n",
            "similar_doc_ind [31, 361, 368]\n",
            "similar_doc_ind [33, 237, 427]\n",
            "similar_doc_ind [34, 36, 220]\n",
            "similar_doc_ind [35, 222, 371]\n",
            "similar_doc_ind [37, 49, 41]\n",
            "similar_doc_ind [38, 432, 145]\n",
            "similar_doc_ind [39, 75, 347]\n",
            "similar_doc_ind [42, 205, 73]\n",
            "similar_doc_ind [43, 105, 331]\n",
            "similar_doc_ind [44, 231, 480]\n",
            "similar_doc_ind [45, 100, 481]\n",
            "similar_doc_ind [46, 466, 255]\n",
            "similar_doc_ind [47, 452, 128]\n",
            "similar_doc_ind [48, 385, 289]\n",
            "similar_doc_ind [50, 309, 272]\n",
            "similar_doc_ind [51, 101, 440]\n",
            "similar_doc_ind [52, 396, 280]\n",
            "similar_doc_ind [53, 259, 258]\n",
            "similar_doc_ind [54, 324, 185]\n",
            "similar_doc_ind [55, 181, 174]\n",
            "similar_doc_ind [56, 477, 215]\n",
            "similar_doc_ind [57, 256, 112]\n",
            "similar_doc_ind [58, 423, 411]\n",
            "similar_doc_ind [59, 195, 265]\n",
            "similar_doc_ind [60, 233, 456]\n",
            "similar_doc_ind [61, 482, 276]\n",
            "similar_doc_ind [64, 311, 261]\n",
            "similar_doc_ind [65, 490, 104]\n",
            "similar_doc_ind [66, 444, 346]\n",
            "similar_doc_ind [67, 457, 310]\n",
            "similar_doc_ind [68, 69, 381]\n",
            "similar_doc_ind [71, 399, 401]\n",
            "similar_doc_ind [72, 159, 160]\n",
            "similar_doc_ind [74, 143, 400]\n",
            "similar_doc_ind [76, 433, 151]\n",
            "similar_doc_ind [77, 93, 236]\n",
            "similar_doc_ind [78, 248, 197]\n",
            "similar_doc_ind [79, 460, 244]\n",
            "similar_doc_ind [80, 81, 162]\n",
            "similar_doc_ind [82, 223, 118]\n",
            "similar_doc_ind [83, 283, 148]\n",
            "similar_doc_ind [84, 228, 351]\n",
            "similar_doc_ind [85, 161, 122]\n",
            "similar_doc_ind [86, 349, 117]\n",
            "similar_doc_ind [87, 249, 182]\n",
            "similar_doc_ind [89, 436, 147]\n",
            "similar_doc_ind [90, 494, 278]\n",
            "similar_doc_ind [91, 251, 496]\n",
            "similar_doc_ind [94, 454, 295]\n",
            "similar_doc_ind [95, 397, 240]\n",
            "similar_doc_ind [96, 98, 362]\n",
            "similar_doc_ind [97, 203, 488]\n",
            "similar_doc_ind [99, 363, 260]\n",
            "similar_doc_ind [102, 191, 298]\n",
            "similar_doc_ind [103, 294, 333]\n",
            "similar_doc_ind [107, 478, 479]\n",
            "similar_doc_ind [108, 483, 414]\n",
            "similar_doc_ind [109, 386, 415]\n",
            "similar_doc_ind [110, 398, 359]\n",
            "similar_doc_ind [113, 291, 491]\n",
            "similar_doc_ind [114, 115, 369]\n",
            "similar_doc_ind [116, 459, 154]\n",
            "similar_doc_ind [119, 453, 308]\n",
            "similar_doc_ind [120, 323, 142]\n",
            "similar_doc_ind [121, 241, 140]\n",
            "similar_doc_ind [123, 388, 189]\n",
            "similar_doc_ind [124, 430, 239]\n",
            "similar_doc_ind [125, 131, 230]\n",
            "similar_doc_ind [127, 277, 319]\n",
            "similar_doc_ind [129, 226, 382]\n",
            "similar_doc_ind [130, 164, 266]\n",
            "similar_doc_ind [132, 465, 330]\n",
            "similar_doc_ind [133, 227, 149]\n",
            "similar_doc_ind [134, 406, 268]\n",
            "similar_doc_ind [135, 184, 144]\n",
            "similar_doc_ind [138, 194, 306]\n",
            "similar_doc_ind [139, 435, 176]\n",
            "similar_doc_ind [141, 357, 288]\n",
            "similar_doc_ind [150, 447, 299]\n",
            "similar_doc_ind [155, 284, 262]\n",
            "similar_doc_ind [156, 211, 329]\n",
            "similar_doc_ind [165, 345, 493]\n",
            "similar_doc_ind [166, 495, 348]\n",
            "similar_doc_ind [167, 469, 379]\n",
            "similar_doc_ind [168, 407, 292]\n",
            "similar_doc_ind [169, 408, 254]\n",
            "similar_doc_ind [170, 429, 364]\n",
            "similar_doc_ind [171, 467, 221]\n",
            "similar_doc_ind [172, 192, 336]\n",
            "similar_doc_ind [173, 253, 317]\n",
            "similar_doc_ind [175, 177, 389]\n",
            "similar_doc_ind [179, 209, 198]\n",
            "similar_doc_ind [183, 463, 431]\n",
            "similar_doc_ind [187, 247, 416]\n",
            "similar_doc_ind [188, 282, 270]\n",
            "similar_doc_ind [190, 376, 439]\n",
            "similar_doc_ind [193, 417, 322]\n",
            "similar_doc_ind [196, 269, 343]\n",
            "similar_doc_ind [199, 373, 449]\n",
            "similar_doc_ind [201, 404, 458]\n",
            "similar_doc_ind [202, 245, 473]\n",
            "similar_doc_ind [204, 438, 354]\n",
            "similar_doc_ind [206, 390, 257]\n",
            "similar_doc_ind [207, 468, 250]\n",
            "similar_doc_ind [208, 360, 325]\n",
            "similar_doc_ind [212, 402, 365]\n",
            "similar_doc_ind [214, 421, 356]\n",
            "similar_doc_ind [216, 314, 405]\n",
            "similar_doc_ind [217, 271, 428]\n",
            "similar_doc_ind [219, 442, 316]\n",
            "similar_doc_ind [225, 337, 498]\n",
            "similar_doc_ind [229, 252, 342]\n",
            "similar_doc_ind [232, 275, 437]\n",
            "similar_doc_ind [238, 321, 450]\n",
            "similar_doc_ind [242, 472, 307]\n",
            "similar_doc_ind [246, 455, 484]\n",
            "similar_doc_ind [263, 287, 492]\n",
            "similar_doc_ind [267, 279, 320]\n",
            "similar_doc_ind [273, 297, 464]\n",
            "similar_doc_ind [281, 448, 372]\n",
            "similar_doc_ind [290, 445, 378]\n",
            "similar_doc_ind [296, 334, 375]\n",
            "similar_doc_ind [300, 499, 394]\n",
            "similar_doc_ind [301, 476, 318]\n",
            "similar_doc_ind [304, 426, 418]\n",
            "similar_doc_ind [313, 424, 358]\n",
            "similar_doc_ind [315, 377, 387]\n",
            "similar_doc_ind [327, 419, 412]\n",
            "similar_doc_ind [338, 500, 355]\n",
            "similar_doc_ind [340, 497, 341]\n",
            "similar_doc_ind [350, 446, 489]\n",
            "similar_doc_ind [353, 451, 392]\n",
            "similar_doc_ind [374, 413, 422]\n",
            "similar_doc_ind [380, 391, 441]\n",
            "similar_doc_ind [383, 425, 461]\n",
            "similar_doc_ind [384, 485, 443]\n",
            "similar_doc_ind [409, 23, 385]\n",
            "Breaking! \tlen(used_indexes) 499 \tlen(docs) 501 \tlen(docs)-k 498\n",
            "CPU times: user 37min 42s, sys: 3.84 s, total: 37min 46s\n",
            "Wall time: 38min 25s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "annon_sents = run_anonymization_on_txt(train_sentences[:501], k=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For running on 1000 documents\n",
        "print_example([0, 546, 933])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1qTgDmy9y0R",
        "outputId": "97740354-e450-40e2-df63-ce53b5d8ee5b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "textbook: book shipped quickly and was in excellent condition as stated. easy transaction would buy again\n",
            "mathbook: excellent condition of book. description is true to the condition of the mathbook.shipped quickly excellent seller.\n",
            "good deal!: used library book, but still in good condition. book came quickly and was cheap. overall good deal!\n",
            "After:\n",
            "used library book, but still in good condition. book came quickly and was cheap. overall good deal!\n",
            "used library book, but still in good condition. book came quickly and was cheap. overall good deal!\n",
            "used library book, but still in good condition. book came quickly and was cheap. overall good deal!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_example([0, 32, 178])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGzPIZCIkleo",
        "outputId": "8990f128-c003-468f-ba70-d516890139f1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "textbook: book shipped quickly and was in excellent condition as stated. easy transaction would buy again\n",
            "recent purchase: the book i ordered was exactly as described and delivery was even faster than promised.\n",
            "great: i received this book promptly and at a good price. i definitely appreciate that kind of service.\n",
            "After:\n",
            "book shipped quickly and was in excellent condition as stated. easy transaction would buy again.\n",
            "book shipped quickly and was in excellent condition as stated. easy transaction would buy again.\n",
            "book shipped quickly and was in excellent condition as stated. easy transaction would buy again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_example([1, 200, 487])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ2MKGXAoqQp",
        "outputId": "0d45dc0c-c852-4961-ec2f-1ac1e00a96aa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "janes all the worlds aircraft 1996-7: great to deal with. very quick delivery. very highly recommended. thank you.\n",
            "great service: great customer service and shipping was quick product in great condition. love the movie. thanks.\n",
            "fantastic: great service, quick shipping, no hassles, item was in perfect condition. no complaints at all.\n",
            "After:\n",
            "janes all the worlds aircraft 1996-7: great to deal with. very quick delivery. very highly recommended.\n",
            "janes all the worlds aircraft 1996-7: great to deal with. very quick delivery. very highly recommended.\n",
            "janes all the worlds aircraft 1996-7: great to deal with. very quick delivery. very highly recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_example([2, 393, 158])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njj-fqZL01Ds",
        "outputId": "6378523e-1a15-4666-9911-7b1a7fcd71be"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "edge of danger: 1 star - only because that's the minimum.this book proves the famous can publish anything.\n",
            "the five star ratings don't lie on this one: an absolute masterpiece.... there's nothing more to be said\n",
            "if ever a book deserved an 11...: absolutely wonderful...surely one of the greatest novels to come out from america.\n",
            "After:\n",
            "edge of danger is rated 1 star - only because that's the minimum. 5 star ratings don't lie on this one: an\n",
            "edge of danger is rated 1 star - only because that's the minimum. 5 star ratings don't lie on this one: an\n",
            "edge of danger is rated 1 star - only because that's the minimum. 5 star ratings don't lie on this one: an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Another example for summary"
      ],
      "metadata": {
        "id": "dNiGRhMzFMYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")"
      ],
      "metadata": {
        "id": "cwOCflupU2yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the input sentences\n",
        "input_text = \"1: textbook: book shipped quickly and was in excellent condition as stated. easy transaction would buy again. 2: recent purchase: the book i ordered was exactly as described and delivery was even faster than promised. 3: recommend this seller: i received this book on time and in excellent condition. i'd definitely recommend this seller.\"\n",
        "#input_text = \"disappointed: this is what happens when artists get too ambitious, ohh man i'm gonna miss the old nelly. curious, so i bought it: another trying hard wannabe dance diva. she should stick to singing ballads. tangerine dream is great.: i love tangerine dream, but this album is not my favorite. still worth owning though.\"\n",
        "input_text = \"1: janes all the worlds aircraft 1996-7: great to deal with. very quick delivery. very highly recommended. thank you. 2: good bike, bad packing: when i received the bike, the packing board was broken.the bike quality is good. 3: great purchase: installed in seconds. great performance. excellent to eliminate phone line dependency.\"\n",
        "\n",
        "# preprocess the input sentences\n",
        "input_ids = tokenizer.encode(\"summarize the 3 documents:\" + input_text, return_tensors=\"pt\")\n",
        "\n",
        "# generate the summary sentence\n",
        "output_ids = model.generate(input_ids=input_ids, max_length=32, num_beams=4, early_stopping=True)\n",
        "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX4-9NxhFL5M",
        "outputId": "8c889724-d587-46d6-8e07-f4d1faf46e3c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "janes all the worlds aircraft 1996-7: great deal with very quick delivery. when i received the bike, the packing board was broken.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cYOu21HzQpA"
      },
      "source": [
        "## Test anonymity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ],
      "metadata": {
        "id": "hCWOUcFh3Y-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uwQGT-mfzB3-"
      },
      "outputs": [],
      "source": [
        "# Credit: https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial \n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
        "\n",
        "def cleaning(doc):\n",
        "    # Defining the document\n",
        "    doc = nlp(doc) \n",
        "\n",
        "    # Lemmatizes and removes stopwords\n",
        "    # doc needs to be a spacy Doc object\n",
        "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    \n",
        "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
        "    # if a sentence is only one or two words long,\n",
        "    # the benefit for the training is very small\n",
        "    #if len(txt) > 2:\n",
        "    #    return ' '.join(txt)\n",
        "    clean_doc = ' '.join(txt)\n",
        "    return clean_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "m97680_5y50Z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_corpus(corpus):\n",
        "    \"\"\" Cleans the corpus \"\"\"\n",
        "    brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in corpus)\n",
        "    corpus_lemmas = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
        "    return corpus_lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "J7n5Jk1jx4Ot"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CountVectorizer is defined only once\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
        "                           stop_words='english')\n",
        "\n",
        "def get_pesonal_docs(docs, min_k = None):\n",
        "    \"\"\" If K not given, returns the minimal current k and the corresponding documents.\n",
        "        If k is given, return the documents with k or less neighbohrs  \"\"\"\n",
        "    \n",
        "    # Lemmatizing the documents\n",
        "    ldocs = clean_corpus(docs)\n",
        "\n",
        "    # Vectorizing\n",
        "    count_data = vectorizer.fit_transform(ldocs)\n",
        "    \n",
        "    # Counting unique values\n",
        "    uniq_arr, uniq_cnt = np.unique(count_data.toarray(), axis=0, return_counts=True)\n",
        "    if not min_k:\n",
        "        min_k = min(uniq_cnt)\n",
        "    \n",
        "    # All the unique vectors\n",
        "    un_anon = uniq_arr[uniq_cnt <= min_k]\n",
        "\n",
        "    # Getting the unique vectore indeces\n",
        "    indeces_list = []\n",
        "    for row in un_anon:\n",
        "        # Get the similar rows\n",
        "        similar_vals = np.where((count_data.toarray() == (row)).all(axis=1))\n",
        "        indeces_list.append(similar_vals[0].tolist())\n",
        "\n",
        "    return min_k, indeces_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CountVectorizer is defined only once\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
        "                           stop_words='english')\n",
        "\n",
        "def get_anonym_degree(docs, min_k = None):\n",
        "    \"\"\" If K not given, returns the minimal current k and the corresponding documents.\n",
        "        If k is given, return the documents with k or less neighbohrs  \"\"\"\n",
        "    \n",
        "    # Lemmatizing the documents\n",
        "    ldocs = clean_corpus(docs)\n",
        "\n",
        "    # Vectorizing\n",
        "    count_data = vectorizer.fit_transform(ldocs)\n",
        "    count_data = count_data.toarray()\n",
        "    # Converting any number larger than 1 into 1\n",
        "    count_data[count_data > 1] = 1\n",
        "    # Counting unique values\n",
        "    uniq_arr, uniq_cnt = np.unique(count_data, axis=0, return_counts=True)\n",
        "    if not min_k:\n",
        "        min_k = min(uniq_cnt)\n",
        "        # All the unique vectors\n",
        "        un_anon = uniq_arr[uniq_cnt <= min_k]\n",
        "    else:\n",
        "        # All the unique vectors\n",
        "        un_anon = uniq_arr[uniq_cnt < min_k]\n",
        "        min_k = min(uniq_cnt) # For the return value\n",
        "\n",
        "    # Getting the unique vectore indeces\n",
        "    indeces_list = []\n",
        "    for row in un_anon:\n",
        "        # Get the similar rows\n",
        "        similar_vals = np.where((count_data == (row)).all(axis=1))\n",
        "        indeces_list.append(similar_vals[0].tolist())\n",
        "\n",
        "    return min_k, indeces_list"
      ],
      "metadata": {
        "id": "pDlq8sz9nkZM"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqMzTqzSck75"
      },
      "source": [
        "### Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "o8--ZiClzGtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c6ec48-3f57-496c-b967-5b5d0fb3c3af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, [[420], [434]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "get_anonym_degree(annon_sents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_anonym_degree(annon_sents, min_k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hxBQ6A369EY",
        "outputId": "0c3f4b6f-a8cd-4b6f-fca0-a2588af7b280"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, [[210, 366], [48, 289], [420], [434]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sR5uqmTNX-x"
      },
      "source": [
        "## GPT-Chat output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lW3zww4NaY-"
      },
      "source": [
        "**Input**:   \n",
        "* janes all the worlds aircraft 1996-7: great to deal with. very quick delivery. very highly recommended. thank you.\n",
        "* good bike, bad packing: when i received the bike, the packing board was broken.the bike quality is good.\n",
        "* great purchase: installed in seconds. great performance. excellent to eliminate phone line dependency.\n",
        "\n",
        "**Output**:   \n",
        "\"The reviews indicate positive experiences with quick delivery and great performance of the products, but with some issues in packaging.\"\n",
        "\n",
        "____\n",
        "\n",
        "**Input**:   \n",
        "* textbook: book shipped quickly and was in excellent condition as stated. easy transaction would buy again\n",
        "* recent purchase: the book i ordered was exactly as described and delivery was even faster than promised.\n",
        "* recommend this seller: i recieved this book on time and in excellent condition. i'd definitely recommend this seller.\n",
        "\n",
        "**Output**:   \n",
        "\"The reviews are positive and recommend the seller for their fast shipping and excellent condition of the books.\"\n",
        "____\n",
        "\n",
        "**Input**:   \n",
        "* beach boys and the satan: a totally off and enjoyable movie. all brian wilson fans should see this movie.\n",
        "* great: one of adams best films along side the wedding singer and mr. deeds. funny movie. worth buying\n",
        "* solid follow up: very good movie. for children 7 and up...any younger they may not grasp the concept.\n",
        "\n",
        "**Output**:   \n",
        "\"The reviews are positive and recommend the movies for their enjoyable and entertaining qualities.\"\t\t"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "1aTCQGFkgjti",
        "rfHyLIe1gWYD",
        "gQDNKaefk65r",
        "M7JgzJm-UxDx"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}